{
  "config": {
    "step": {
      "user": {
        "title": "OpenRouter API Configuration",
        "description": "Enter your OpenRouter API key. You can get one at {docs_url}\n\nAfter setup, add Conversation Agents or AI Tasks via the integration menu.",
        "data": {
          "api_key": "API Key"
        }
      }
    },
    "error": {
      "invalid_api_key": "Invalid API key. Please check and try again.",
      "cannot_connect": "Cannot connect to OpenRouter API.",
      "unknown": "An unexpected error occurred."
    },
    "abort": {
      "already_configured": "Smart Assist is already configured."
    }
  },
  "options": {
    "step": {
      "init": {
        "title": "Smart Assist Settings",
        "description": "Configure global settings for Smart Assist.",
        "data": {
          "debug_logging": "Enable debug logging"
        },
        "data_description": {
          "debug_logging": "Enable verbose logging for debugging."
        }
      }
    }
  },
  "config_subentries": {
    "conversation": {
      "initiate_flow": {
        "user": "Add Conversation Agent"
      },
      "step": {
        "user": {
          "title": "Model Selection",
          "description": "Select the AI model for this conversation agent.\n\nProvider will be set to 'Automatic' by default. You can change it later via reconfigure.",
          "data": {
            "model": "Model",
            "temperature": "Temperature (0 = precise, 1 = creative)",
            "max_tokens": "Maximum response tokens"
          },
          "data_description": {
            "model": "Select an OpenRouter model or enter a custom model ID.",
            "temperature": "Lower values produce more consistent outputs, higher values are more creative.",
            "max_tokens": "Maximum number of tokens in the response."
          }
        },
        "behavior": {
          "title": "Behavior Settings",
          "description": "Configure how this conversation agent behaves.",
          "data": {
            "language": "Response Language",
            "exposed_only": "Use only exposed entities",
            "confirm_critical": "Confirm critical actions (locks, alarms)",
            "max_history": "Maximum conversation history",
            "enable_web_search": "Enable web search (DuckDuckGo)",
            "enable_prompt_caching": "Enable prompt caching (reduces latency and cost)",
            "cache_ttl_extended": "Extended cache TTL (1 hour for Anthropic)",
            "enable_cache_warming": "Enable automatic cache warming",
            "cache_refresh_interval": "Cache refresh interval (minutes)",
            "clean_responses": "Clean responses for voice output",
            "ask_followup": "Ask follow-up questions"
          },
          "data_description": {
            "cache_ttl_extended": "Enable 1-hour cache instead of 5 minutes. Only applies to Anthropic models.",
            "enable_cache_warming": "Keeps the prompt cache warm by sending periodic requests. Incurs small additional API costs.",
            "cache_refresh_interval": "How often to refresh the cache (in minutes). Set below the cache TTL.",
            "clean_responses": "Makes responses TTS-friendly by removing emojis, markdown formatting, and URLs.",
            "ask_followup": "If enabled, the assistant will offer further help after completing actions."
          }
        },
        "prompt": {
          "title": "System Prompt",
          "description": "Customize the assistant's personality and behavior.",
          "data": {
            "user_system_prompt": "System Prompt"
          },
          "data_description": {
            "user_system_prompt": "Personality and instructions for this conversation agent."
          }
        },
        "reconfigure": {
          "title": "Reconfigure Agent",
          "description": "Update settings for this conversation agent.\n\nTo select a specific provider for prompt caching, choose one from the Provider dropdown.",
          "data": {
            "model": "Model",
            "temperature": "Temperature",
            "max_tokens": "Maximum response tokens",
            "provider": "Provider",
            "language": "Response Language",
            "exposed_only": "Use only exposed entities",
            "confirm_critical": "Confirm critical actions",
            "enable_prompt_caching": "Enable prompt caching",
            "cache_ttl_extended": "Extended cache TTL",
            "enable_cache_warming": "Enable cache warming",
            "cache_refresh_interval": "Cache refresh interval",
            "clean_responses": "Clean responses for voice output",
            "ask_followup": "Ask follow-up questions",
            "user_system_prompt": "System Prompt"
          }
        }
      }
    },
    "ai_task": {
      "initiate_flow": {
        "user": "Add AI Task"
      },
      "step": {
        "user": {
          "title": "Model Selection",
          "description": "Select the AI model for this task.",
          "data": {
            "model": "Model",
            "temperature": "Temperature (0 = precise, 1 = creative)",
            "max_tokens": "Maximum response tokens"
          },
          "data_description": {
            "model": "Select an OpenRouter model or enter a custom model ID.",
            "temperature": "Lower values produce more consistent outputs, higher values are more creative.",
            "max_tokens": "Maximum number of tokens in the response."
          }
        },
        "settings": {
          "title": "Task Settings",
          "description": "Configure this AI task.",
          "data": {
            "language": "Response Language",
            "exposed_only": "Use only exposed entities",
            "task_system_prompt": "System Prompt",
            "task_enable_prompt_caching": "Enable prompt caching",
            "task_enable_cache_warming": "Enable cache warming"
          },
          "data_description": {
            "task_system_prompt": "Instructions for this AI task in automations.",
            "task_enable_prompt_caching": "Not recommended - background tasks are not time-critical.",
            "task_enable_cache_warming": "Not recommended - background tasks are not time-critical."
          }
        }
      }
    }
  }
}
