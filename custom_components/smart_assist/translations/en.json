{
  "config": {
    "step": {
      "user": {
        "title": "OpenRouter API Configuration",
        "description": "Enter your OpenRouter API key. You can get one at {docs_url}",
        "data": {
          "api_key": "API Key"
        }
      },
      "model": {
        "title": "Model Configuration",
        "description": "Select any OpenRouter model. You can enter a custom model ID or choose from suggestions.\n\n**Prompt Caching:** If enabled, select a specific provider (not 'auto'). Not all models support caching - see [OpenRouter docs]({caching_docs_url}).",
        "data": {
          "model": "Model (e.g. anthropic/claude-3-haiku)",
          "provider": "Provider",
          "temperature": "Temperature (0 = precise, 1 = creative)",
          "max_tokens": "Maximum response tokens"
        },
        "data_description": {
          "provider": "For prompt caching, select a specific provider. 'Auto' may route to providers without caching support."
        }
      },
      "behavior": {
        "title": "Behavior Settings",
        "description": "Configure how Smart Assist behaves",
        "data": {
          "language": "Response Language",
          "exposed_only": "Use only exposed entities",
          "confirm_critical": "Confirm critical actions (locks, alarms)",
          "max_history": "Maximum conversation history",
          "enable_web_search": "Enable web search (DuckDuckGo)",
          "enable_quick_actions": "Enable quick actions (bypass LLM for simple commands)",
          "enable_prompt_caching": "Enable prompt caching (reduces latency and cost)",
          "cache_ttl_extended": "Extended cache TTL (1 hour for Anthropic)",
          "enable_cache_warming": "Enable automatic cache warming (incurs additional cost)",
          "cache_refresh_interval": "Cache refresh interval",
          "clean_responses": "Clean responses for voice output",
          "ask_followup": "Ask follow-up questions"
        },
        "data_description": {
          "cache_ttl_extended": "Enable 1-hour cache instead of 5 minutes. Reduces costs for frequently used prompts. Only applies to Anthropic models.",
          "enable_cache_warming": "Keeps the prompt cache warm by sending periodic requests. This ensures instant responses but incurs small additional API costs (~1 request per interval).",
          "cache_refresh_interval": "How often to refresh the cache (in minutes). Set below the cache TTL (5 min standard, 60 min extended). Lower = more reliable cache, higher cost.",
          "clean_responses": "Makes responses TTS-friendly by removing emojis, markdown formatting, and URLs. Converts symbols to words (e.g., 25Â°C to '25 degrees Celsius'). Original responses are preserved in conversation history.",
          "ask_followup": "If enabled, the assistant will offer further help after completing actions. Disable for voice assistants to avoid loop-like interactions."
        }
      },
      "prompt": {
        "title": "Prompt Configuration",
        "description": "Customize the assistant's personality and behavior",
        "data": {
          "user_system_prompt": "Custom System Prompt"
        }
      }
    },
    "error": {
      "invalid_api_key": "Invalid API key. Please check and try again.",
      "cannot_connect": "Cannot connect to OpenRouter API.",
      "unknown": "An unexpected error occurred."
    },
    "abort": {
      "already_configured": "Smart Assist is already configured."
    }
  },
  "options": {
    "step": {
      "init": {
        "title": "Smart Assist Settings",
        "description": "Adjust your Smart Assist configuration.\n\n**Prompt Caching:** Requires a specific provider (not 'auto') and a supported model.",
        "data": {
          "model": "Model",
          "provider": "Provider",
          "temperature": "Temperature",
          "max_tokens": "Maximum response tokens",
          "language": "Response Language",
          "exposed_only": "Use only exposed entities",
          "confirm_critical": "Confirm critical actions",
          "max_history": "Maximum conversation history",
          "enable_web_search": "Enable web search",
          "enable_quick_actions": "Enable quick actions",
          "enable_prompt_caching": "Enable prompt caching",
          "cache_ttl_extended": "Extended cache TTL (1h for Anthropic)",
          "enable_cache_warming": "Enable cache warming (incurs cost)",
          "cache_refresh_interval": "Cache refresh interval (min)",
          "clean_responses": "Clean responses for voice output",
          "ask_followup": "Ask follow-up questions",
          "user_system_prompt": "Custom System Prompt"
        },
        "data_description": {
          "provider": "For prompt caching, select a specific provider. Not all models support caching.",
          "enable_prompt_caching": "Reduces latency and cost. Requires supported model + specific provider."
        }
      }
    }
  }
}
