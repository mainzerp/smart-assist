{
  "config": {
    "step": {
      "user": {
        "title": "Select LLM Provider",
        "description": "Choose your preferred LLM provider.\n\n**Groq**: Ultra-fast inference with automatic caching (recommended)\n**OpenRouter**: Access to all models (Claude, GPT-4, Llama, etc.)\n**Ollama**: Local, private inference (no cloud, no API key needed)",
        "data": {
          "llm_provider": "LLM Provider"
        }
      },
      "api_key": {
        "title": "API Key",
        "description": "Enter your API key. You can get one at {docs_url}",
        "data": {
          "api_key": "OpenRouter API Key",
          "groq_api_key": "Groq API Key",
          "ollama_url": "Ollama Server URL",
          "ollama_model": "Ollama Model",
          "ollama_keep_alive": "Keep Model Loaded",
          "ollama_num_ctx": "Context Window Size",
          "ollama_timeout": "Request Timeout (seconds)"
        },
        "data_description": {
          "ollama_url": "URL of your Ollama server (default: http://localhost:11434)",
          "ollama_model": "Select a model from your local Ollama installation",
          "ollama_keep_alive": "How long to keep the model in memory (-1 = forever)",
          "ollama_num_ctx": "Context window size in tokens (default: 8192)",
          "ollama_timeout": "Maximum time to wait for a response (default: 120s)"
        }
      },
      "reconfigure": {
        "title": "Update API Keys & Ollama Settings",
        "description": "Update your API keys and Ollama settings. Leave API key fields empty to keep existing values.\n\n**Groq API Key**: {groq_status}\n**OpenRouter API Key**: {openrouter_status}\n**Ollama URL**: {ollama_status}",
        "data": {
          "groq_api_key": "New Groq API Key",
          "api_key": "New OpenRouter API Key",
          "ollama_url": "New Ollama URL",
          "ollama_num_ctx": "Context Window Size",
          "ollama_keep_alive": "Keep Model Loaded",
          "ollama_timeout": "Request Timeout (seconds)"
        },
        "data_description": {
          "groq_api_key": "Enter a new Groq API key or leave empty to keep the current one.",
          "api_key": "Enter a new OpenRouter API key or leave empty to keep the current one.",
          "ollama_url": "Enter a new Ollama URL or leave empty to keep the current one.",
          "ollama_num_ctx": "Context window size in tokens. Larger = more context but more RAM.",
          "ollama_keep_alive": "How long to keep the model loaded in memory after a request.",
          "ollama_timeout": "Maximum time to wait for Ollama to respond (in seconds)."
        }
      }
    },
    "error": {
      "invalid_api_key": "Invalid OpenRouter API key. Please check and try again.",
      "invalid_groq_api_key": "Invalid Groq API key. Please check and try again.",
      "ollama_connection_failed": "Cannot connect to Ollama server. Make sure Ollama is running at the specified URL.",
      "ollama_not_configured": "Ollama is not configured. Please add Ollama configuration in the main integration settings.",
      "groq_api_key_required": "Groq API key is required for Groq provider.",
      "openrouter_api_key_required": "OpenRouter API key is required for OpenRouter provider.",
      "no_api_key": "At least one API key (OpenRouter or Groq) is required.",
      "cannot_connect": "Cannot connect to API.",
      "unknown": "An unexpected error occurred."
    },
    "abort": {
      "already_configured": "Smart Assist is already configured."
    }
  },
  "options": {
    "step": {
      "init": {
        "title": "Smart Assist Settings",
        "description": "Configure global settings for Smart Assist.",
        "data": {
          "debug_logging": "Enable debug logging"
        },
        "data_description": {
          "debug_logging": "Enable verbose logging for debugging."
        }
      }
    }
  },
  "config_subentries": {
    "conversation": {
      "entry_type": "Conversation agent",
      "initiate_flow": {
        "user": "Add Conversation Agent",
        "reconfigure": "Reconfigure Conversation Agent"
      },
      "step": {
        "user": {
          "title": "LLM Provider Selection",
          "description": "Select the LLM provider for this conversation agent.\n\n**OpenRouter**: Access to all models (Claude, GPT-4, Llama, etc.) via a unified API.\n**Groq**: Direct connection for ultra-fast inference with automatic caching.\n**Ollama**: Local, private inference without cloud dependency.",
          "data": {
            "llm_provider": "LLM Provider",
            "groq_api_key": "Groq API Key"
          },
          "data_description": {
            "llm_provider": "Select OpenRouter for model variety, Groq for fastest inference, or Ollama for local privacy.",
            "groq_api_key": "Enter your Groq API key (get one at console.groq.com)."
          }
        },
        "model": {
          "title": "Model Selection",
          "description": "Select the AI model for this conversation agent.",
          "data": {
            "model": "Model"
          },
          "data_description": {
            "model": "Select a model or enter a custom model ID."
          }
        },
        "settings": {
          "title": "Agent Settings",
          "description": "Configure provider and behavior settings for this conversation agent.",
          "data": {
            "provider": "Provider Routing",
            "temperature": "Temperature (0 = precise, 1 = creative)",
            "max_tokens": "Maximum response tokens",
            "language": "Response Language",
            "exposed_only": "Use only exposed entities",
            "confirm_critical": "Confirm critical actions (locks, alarms)",
            "max_history": "Maximum conversation history",
            "enable_web_search": "Enable web search (DuckDuckGo)",
            "enable_prompt_caching": "Enable prompt caching (reduces latency and cost)",
            "cache_ttl_extended": "Extended cache TTL (1 hour for Anthropic)",
            "enable_cache_warming": "Enable automatic cache warming",
            "cache_refresh_interval": "Cache refresh interval (minutes)",
            "clean_responses": "Clean responses for voice output",
            "ask_followup": "Ask follow-up questions",
            "calendar_context": "Proactive calendar reminders"
          },
          "data_description": {
            "provider": "OpenRouter only: Select a specific provider or use 'Automatic' for best price routing.",
            "temperature": "Lower values produce more consistent outputs, higher values are more creative.",
            "max_tokens": "Maximum number of tokens in the response.",
            "language": "Leave empty for auto-detect (uses HA language). Or enter any language (e.g., 'French', 'es-ES', 'Japanese').",
            "cache_ttl_extended": "Enable 1-hour cache instead of 5 minutes. Only applies to Anthropic models.",
            "enable_cache_warming": "Keeps the prompt cache warm by sending periodic requests. Incurs small additional API costs.",
            "cache_refresh_interval": "How often to refresh the cache (in minutes). Set below the cache TTL.",
            "clean_responses": "Makes responses TTS-friendly by removing emojis, markdown formatting, and URLs.",
            "ask_followup": "If enabled, the assistant will offer further help after completing actions.",
            "calendar_context": "Automatically mention upcoming calendar events. Note: Increases token usage."
          }
        },
        "prompt": {
          "title": "System Prompt",
          "description": "Customize the assistant's personality and behavior.",
          "data": {
            "user_system_prompt": "System Prompt"
          },
          "data_description": {
            "user_system_prompt": "Personality and instructions for this conversation agent."
          }
        },
        "reconfigure": {
          "title": "Reconfigure Agent - LLM Provider",
          "description": "Select the LLM provider for this agent.\n\nAfter confirming, you can update model and other settings.",
          "data": {
            "llm_provider": "LLM Provider",
            "groq_api_key": "Groq API Key"
          }
        },
        "reconfigure_model": {
          "title": "Reconfigure Agent - Model",
          "description": "Select a new model for this agent.",
          "data": {
            "model": "Model"
          }
        },
        "reconfigure_settings": {
          "title": "Reconfigure Agent - Settings",
          "description": "Update provider and behavior settings for this conversation agent.",
          "data": {
            "provider": "Provider Routing",
            "temperature": "Temperature",
            "max_tokens": "Maximum response tokens",
            "language": "Response Language",
            "exposed_only": "Use only exposed entities",
            "confirm_critical": "Confirm critical actions",
            "max_history": "Maximum conversation history",
            "enable_web_search": "Enable web search",
            "enable_prompt_caching": "Enable prompt caching",
            "cache_ttl_extended": "Extended cache TTL",
            "enable_cache_warming": "Enable cache warming",
            "cache_refresh_interval": "Cache refresh interval",
            "clean_responses": "Clean responses for voice output",
            "ask_followup": "Ask follow-up questions",
            "calendar_context": "Proactive calendar reminders",
            "user_system_prompt": "System Prompt"
          }
        }
      },
      "abort": {
        "reconfigure_successful": "Settings updated successfully.",
        "no_api_keys_configured": "No API keys configured. Please add at least one API key in the main integration settings."
      },
      "error": {
        "groq_api_key_required": "Groq API key is required for Groq provider.",
        "openrouter_api_key_required": "OpenRouter API key is required for OpenRouter provider.",
        "ollama_not_configured": "Ollama is not configured. Please add Ollama configuration in the main integration settings."
      }
    },
    "ai_task": {
      "entry_type": "AI Task",
      "initiate_flow": {
        "user": "Add AI Task",
        "reconfigure": "Reconfigure AI Task"
      },
      "step": {
        "user": {
          "title": "LLM Provider Selection",
          "description": "Select the LLM provider for this AI task.\n\n**OpenRouter**: Access to all models via a unified API.\n**Groq**: Direct connection for ultra-fast inference.",
          "data": {
            "llm_provider": "LLM Provider",
            "groq_api_key": "Groq API Key"
          },
          "data_description": {
            "llm_provider": "Select OpenRouter for model variety or Groq for fastest inference.",
            "groq_api_key": "Enter your Groq API key (get one at console.groq.com)."
          }
        },
        "model": {
          "title": "Model Selection",
          "description": "Select the AI model for this task.",
          "data": {
            "model": "Model"
          },
          "data_description": {
            "model": "Select a model or enter a custom model ID."
          }
        },
        "settings": {
          "title": "Task Settings",
          "description": "Configure provider and other settings for this AI task.",
          "data": {
            "provider": "Provider Routing",
            "temperature": "Temperature (0 = precise, 1 = creative)",
            "max_tokens": "Maximum response tokens",
            "language": "Response Language",
            "exposed_only": "Use only exposed entities",
            "task_system_prompt": "System Prompt",
            "task_enable_prompt_caching": "Enable prompt caching",
            "task_enable_cache_warming": "Enable cache warming"
          },
          "data_description": {
            "provider": "OpenRouter only: Select a specific provider or use 'Automatic' for best price routing.",
            "temperature": "Lower values produce more consistent outputs, higher values are more creative.",
            "max_tokens": "Maximum number of tokens in the response.",
            "language": "Leave empty for auto-detect (uses HA language). Or enter any language (e.g., 'French', 'es-ES', 'Japanese').",
            "task_system_prompt": "Instructions for this AI task in automations.",
            "task_enable_prompt_caching": "Not recommended - background tasks are not time-critical.",
            "task_enable_cache_warming": "Not recommended - background tasks are not time-critical."
          }
        },
        "reconfigure": {
          "title": "Reconfigure Task - LLM Provider",
          "description": "Select the LLM provider for this task.\n\nAfter confirming, you can update model and other settings.",
          "data": {
            "llm_provider": "LLM Provider",
            "groq_api_key": "Groq API Key"
          }
        },
        "reconfigure_model": {
          "title": "Reconfigure Task - Model",
          "description": "Select a new model for this AI task.",
          "data": {
            "model": "Model"
          }
        },
        "reconfigure_settings": {
          "title": "Reconfigure Task - Settings",
          "description": "Update provider and behavior settings for this AI task.",
          "data": {
            "provider": "Provider Routing",
            "temperature": "Temperature",
            "max_tokens": "Maximum response tokens",
            "language": "Response Language",
            "exposed_only": "Use only exposed entities",
            "task_system_prompt": "System Prompt",
            "task_enable_prompt_caching": "Enable prompt caching",
            "task_enable_cache_warming": "Enable cache warming"
          }
        }
      },
      "abort": {
        "reconfigure_successful": "Settings updated successfully.",
        "no_api_keys_configured": "No API keys configured. Please add at least one API key in the main integration settings."
      },
      "error": {
        "groq_api_key_required": "Groq API key is required for Groq provider.",
        "openrouter_api_key_required": "OpenRouter API key is required for OpenRouter provider.",
        "ollama_not_configured": "Ollama is not configured. Please add Ollama configuration in the main integration settings."
      }
    }
  }
}
